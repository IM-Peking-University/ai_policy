{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48527f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def compute_confidence_interval_mean(mean, std, n, confidence=0.95):\n",
    "\n",
    "    if n < 2 or pd.isna(std) or std == 0:\n",
    "        return np.nan, np.nan\n",
    "    sem = std / np.sqrt(n)\n",
    "    ci = stats.t.interval(confidence, df=n-1, loc=mean, scale=sem)\n",
    "    return ci[0], ci[1]\n",
    "\n",
    "def analyze_and_save_group(df, group_by_cols, output_filename):\n",
    "\n",
    "    for col in ['has_ai_policy', 'is_oa']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    grouped = df.groupby(group_by_cols)['ai_word_freq']\n",
    "    \n",
    "    agg_results = grouped.agg(['count', 'mean', 'std']).reset_index()\n",
    "    \n",
    "    ci_results = agg_results.apply(\n",
    "        lambda row: compute_confidence_interval_mean(row['mean'], row['std'], row['count']),\n",
    "        axis=1,\n",
    "        result_type='expand'\n",
    "    )\n",
    "    agg_results[['ci_lower', 'ci_upper']] = ci_results\n",
    "    \n",
    "    agg_results = agg_results.drop(columns=['std'])\n",
    "\n",
    "    agg_results.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_file = r'C:\\Users\\ningji\\Desktop\\ai_policy\\ai_policy\\data\\full_text_keyword_count.csv'\n",
    "    output_dir = r'C:\\Users\\ningji\\Desktop\\ai_policy\\ai_policy\\results\\full_text'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dtype_spec = {\n",
    "        'ai_word_freq': 'int32',\n",
    "        'has_ai_policy': 'boolean',\n",
    "        'is_oa': 'boolean'\n",
    "    }\n",
    "    df = pd.read_csv(input_file, dtype=dtype_spec, parse_dates=['date'])\n",
    "\n",
    "    df.dropna(subset=['date'], inplace=True)\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['half_year'] = df['date'].dt.month.apply(lambda m: 'H1' if m <= 6 else 'H2')\n",
    "    \n",
    "    scenarios = [\n",
    "        # 1. Overall trend by half-year\n",
    "        {'group_cols': ['year', 'half_year'], 'filename': 'by_half_year.csv'},\n",
    "        # 2. Trend by AI policy status\n",
    "        {'group_cols': ['year', 'half_year', 'has_ai_policy'], 'filename': 'by_half_year_policy.csv'},\n",
    "        # 3. Trend by OA status\n",
    "        {'group_cols': ['year', 'half_year', 'is_oa'], 'filename': 'by_half_year_oa.csv'},\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        output_path = os.path.join(output_dir, scenario['filename'])\n",
    "        analyze_and_save_group(df.copy(), scenario['group_cols'], output_path)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a252a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\ningji\\Desktop\\ai_policy\\ai_policy\\data\\full_text_keyword_count.csv\")\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\ningji\\Desktop\\ai_policy\\ai_policy\\results\\full_text\\disclosure_ai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7b897c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "df=df[['paper_id','date','has_ai_policy','journal_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26705d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['doc_id','location','ai_tool','usage_purpose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dff402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversion function\n",
    "def normalize_openalex_id(paper_id):\n",
    "    \"\"\"Convert @https://openalex.org/W4225400651 format to https___openalex.org_W4225400651 format\"\"\"\n",
    "    if pd.isna(paper_id):\n",
    "        return paper_id\n",
    "    return paper_id.replace('@', '').replace('://', '___').replace('/', '_')\n",
    "\n",
    "# Convert paper_id\n",
    "df['paper_id_normalized'] = df['paper_id'].apply(normalize_openalex_id)\n",
    "\n",
    "# Merge data\n",
    "merged_df = pd.merge(df, data, left_on='paper_id_normalized', right_on='doc_id', how='right')\n",
    "\n",
    "# Clean temporary column\n",
    "merged_df = merged_df.drop('paper_id_normalized', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0bcd967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>date</th>\n",
       "      <th>has_ai_policy</th>\n",
       "      <th>location</th>\n",
       "      <th>ai_tool</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>usage_purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W4400975186</td>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>True</td>\n",
       "      <td>Declaration of Interests</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>PLoS ONE</td>\n",
       "      <td>evaluate the capabilities of AI systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W4404702677</td>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>False</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Psychological Medicine</td>\n",
       "      <td>English language editing assistance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           paper_id        date  has_ai_policy  \\\n",
       "0  https://openalex.org/W4400975186  2024-07-25           True   \n",
       "1  https://openalex.org/W4404702677  2024-11-25          False   \n",
       "\n",
       "                   location  ai_tool            journal_name  \\\n",
       "0  Declaration of Interests  ChatGPT                PLoS ONE   \n",
       "1          Acknowledgements   Claude  Psychological Medicine   \n",
       "\n",
       "                             usage_purpose  \n",
       "0  evaluate the capabilities of AI systems  \n",
       "1      English language editing assistance  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df[['paper_id','date','has_ai_policy','location','ai_tool','journal_name','usage_purpose']]\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f7d6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(r'C:\\Users\\ningji\\Desktop\\ai_policy\\ai_policy\\results\\full_text\\disclosure_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d789158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=filtered_df[['paper_id', 'date', 'has_ai_policy', 'location', 'ai_tool',\n",
    "       'journal_name', 'usage_purpose', 'journal_location', 'journal_category',\n",
    "       'journal_usage','journal_usage_categories', 'usage_purpose_categories_filtered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d5e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_id', 'date', 'has_ai_policy', 'location', 'ai_tool',\n",
       "       'journal_name', 'usage_purpose', 'journal_location', 'journal_category',\n",
       "       'journal_usage', 'journal_name_std', 'usage_purpose_items',\n",
       "       'usage_purpose_categories', 'journal_usage_items',\n",
       "       'journal_usage_categories', 'usage_purpose_categories_filtered'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在标准化期刊名称...\n",
      "标准化后的映射字典大小:\n",
      "Location映射: 5113\n",
      "Category映射: 5113\n",
      "Usage映射: 3555\n",
      "\n",
      "标准化匹配后的统计：\n",
      "journal_location 非空值: 113\n",
      "journal_category 非空值: 113\n",
      "journal_usage 非空值: 81\n",
      "\n",
      "匹配率：\n",
      "journal_location 匹配率: 96.6%\n",
      "journal_category 匹配率: 96.6%\n",
      "journal_usage 匹配率: 69.2%\n",
      "\n",
      "前5行对比：\n",
      "             journal_name        journal_name_std journal_location  \\\n",
      "0                PLoS ONE                plos one          Methods   \n",
      "1  Psychological Medicine       psychological med    Not Specified   \n",
      "2  The Historical Journal  the historical journal              NaN   \n",
      "3  JMIR Medical Education      jmir med education    Not Specified   \n",
      "4   BMC Medical Education       bmc med education          Methods   \n",
      "\n",
      "      journal_category  \n",
      "0  Disclosure Required  \n",
      "1        Not Mentioned  \n",
      "2                  NaN  \n",
      "3        Not Mentioned  \n",
      "4  Disclosure Required  \n",
      "\n",
      "仍未匹配location的期刊数量: 4\n",
      "前10个未匹配的期刊:\n",
      "原名: 'The Historical Journal' -> 标准化: 'the historical journal'\n",
      "原名: 'Journal of the Medical Library Association JMLA' -> 标准化: 'j the med library assoc jmla'\n",
      "原名: 'Clinical Chemistry and Laboratory Medicine (CCLM)' -> 标准化: 'clinical chemistry and laboratory med cclm'\n",
      "原名: 'Astronomy and Astrophysics' -> 标准化: 'astronomy and astrophysics'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def standardize_journal_name(name):\n",
    "    \"\"\"Standardize journal name\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    name = str(name)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    name = unicodedata.normalize('NFKD', name)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    \n",
    "    # Remove common punctuation and special characters\n",
    "    name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    # Strip leading and trailing spaces\n",
    "    name = name.strip()\n",
    "    \n",
    "    # Handle common journal name variants\n",
    "    replacements = {\n",
    "        '&': 'and',\n",
    "        'journal of': 'j',\n",
    "        'international journal': 'int j',\n",
    "        'proceedings of': 'proc',\n",
    "        'transactions on': 'trans',\n",
    "        'communications': 'comm',\n",
    "        'conference': 'conf',\n",
    "        'society': 'soc',\n",
    "        'association': 'assoc',\n",
    "        'research': 'res',\n",
    "        'science': 'sci',\n",
    "        'technology': 'tech',\n",
    "        'medicine': 'med',\n",
    "        'medical': 'med',\n",
    "        'engineering': 'eng',\n",
    "        'computer': 'comp',\n",
    "        'international': 'int',\n",
    "        'american': 'am',\n",
    "        'european': 'eur',\n",
    "        'national': 'nat'\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "    \n",
    "    # Clean spaces again\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Recreate standardized mapping dictionary\n",
    "print(\"Standardizing journal names...\")\n",
    "\n",
    "# Standardize JSON data\n",
    "journal_location_map_std = {}\n",
    "journal_category_map_std = {}\n",
    "\n",
    "for journal in journal_info:\n",
    "    journal_name = journal.get('journal_name')\n",
    "    if journal_name:\n",
    "        std_name = standardize_journal_name(journal_name)\n",
    "        journal_location_map_std[std_name] = journal.get('location')\n",
    "        journal_category_map_std[std_name] = journal.get('policy_category')\n",
    "\n",
    "# Standardize CSV data\n",
    "journal_usage_map_std = {}\n",
    "for idx, row in usage_data.iterrows():\n",
    "    journal_name = row['journal_name']\n",
    "    allowed_usage = row['Allowed_Usage']\n",
    "    if pd.notna(journal_name):\n",
    "        std_name = standardize_journal_name(journal_name)\n",
    "        if std_name not in journal_usage_map_std:\n",
    "            journal_usage_map_std[std_name] = allowed_usage\n",
    "\n",
    "print(f\"Standardized mapping dictionary sizes:\")\n",
    "print(f\"Location mapping: {len(journal_location_map_std)}\")\n",
    "print(f\"Category mapping: {len(journal_category_map_std)}\")\n",
    "print(f\"Usage mapping: {len(journal_usage_map_std)}\")\n",
    "\n",
    "# Standardize journal names in merged_df and match\n",
    "merged_df['journal_name_std'] = merged_df['journal_name'].apply(standardize_journal_name)\n",
    "\n",
    "# Re-match data\n",
    "merged_df['journal_location'] = merged_df['journal_name_std'].map(journal_location_map_std)\n",
    "merged_df['journal_category'] = merged_df['journal_name_std'].map(journal_category_map_std)\n",
    "merged_df['journal_usage'] = merged_df['journal_name_std'].map(journal_usage_map_std)\n",
    "\n",
    "# View improved results\n",
    "print(\"\\nStatistics after standardized matching:\")\n",
    "print(f\"journal_location non-null values: {merged_df['journal_location'].notna().sum()}\")\n",
    "print(f\"journal_category non-null values: {merged_df['journal_category'].notna().sum()}\")\n",
    "print(f\"journal_usage non-null values: {merged_df['journal_usage'].notna().sum()}\")\n",
    "\n",
    "print(\"\\nMatching rates:\")\n",
    "print(f\"journal_location matching rate: {merged_df['journal_location'].notna().sum() / len(merged_df) * 100:.1f}%\")\n",
    "print(f\"journal_category matching rate: {merged_df['journal_category'].notna().sum() / len(merged_df) * 100:.1f}%\")\n",
    "print(f\"journal_usage matching rate: {merged_df['journal_usage'].notna().sum() / len(merged_df) * 100:.1f}%\")\n",
    "\n",
    "# View comparison of first few rows\n",
    "print(\"\\nFirst 5 rows comparison:\")\n",
    "comparison = merged_df[['journal_name', 'journal_name_std', 'journal_location', 'journal_category']].head()\n",
    "print(comparison)\n",
    "\n",
    "# Check journals still unmatched\n",
    "unmatched_location = merged_df[merged_df['journal_location'].isna()]\n",
    "if len(unmatched_location) > 0:\n",
    "    print(f\"\\nJournals still unmatched for location: {len(unmatched_location)}\")\n",
    "    print(\"First 10 unmatched journals:\")\n",
    "    for idx, row in unmatched_location[['journal_name', 'journal_name_std']].head(10).iterrows():\n",
    "        print(f\"Original: '{row['journal_name']}' -> Standardized: '{row['journal_name_std']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "061ee973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>date</th>\n",
       "      <th>has_ai_policy</th>\n",
       "      <th>location</th>\n",
       "      <th>ai_tool</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>usage_purpose</th>\n",
       "      <th>journal_location</th>\n",
       "      <th>journal_category</th>\n",
       "      <th>journal_usage</th>\n",
       "      <th>journal_name_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W4400975186</td>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>True</td>\n",
       "      <td>Declaration of Interests</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>PLoS ONE</td>\n",
       "      <td>evaluate the capabilities of AI systems</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>Text content generation</td>\n",
       "      <td>plos one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W4404702677</td>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>False</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Psychological Medicine</td>\n",
       "      <td>English language editing assistance</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>psychological med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W4404494880</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>False</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>Claude</td>\n",
       "      <td>The Historical Journal</td>\n",
       "      <td>revision process</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the historical journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W4376866715</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>JMIR Medical Education</td>\n",
       "      <td>case study to discuss opportunities and challe...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jmir med education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W4408155423</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>True</td>\n",
       "      <td>Contributor Section</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>BMC Medical Education</td>\n",
       "      <td>assistance</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>AI assisted copy editing; Formatting changes; ...</td>\n",
       "      <td>bmc med education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://openalex.org/W4323050332</td>\n",
       "      <td>2023-03-04</td>\n",
       "      <td>True</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>Journal of Medical Systems</td>\n",
       "      <td>answering medical questions</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>General writing assistance</td>\n",
       "      <td>j med systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://openalex.org/W4398223368</td>\n",
       "      <td>2024-05-22</td>\n",
       "      <td>True</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>Grammarly, ChatGPT</td>\n",
       "      <td>PLoS ONE</td>\n",
       "      <td>language improvement, advanced grammar and sty...</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>Text content generation</td>\n",
       "      <td>plos one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://openalex.org/W4404968551</td>\n",
       "      <td>2024-12-03</td>\n",
       "      <td>True</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>ChatGPT-3.5</td>\n",
       "      <td>International Journal of Cancer</td>\n",
       "      <td>language and readability improvement</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>General editing; Grammar improvement; Spelling...</td>\n",
       "      <td>int j cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://openalex.org/W4376611400</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>International Journal for Educational Integrity</td>\n",
       "      <td>assessing quality by comparing seminal ideas o...</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>AI assisted copy editing; Formatting changes; ...</td>\n",
       "      <td>int j for educational integrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://openalex.org/W4396873924</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>True</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>Molecular Breeding</td>\n",
       "      <td>refine the written content of this publication</td>\n",
       "      <td>Methods</td>\n",
       "      <td>Disclosure Required</td>\n",
       "      <td>General writing assistance</td>\n",
       "      <td>molecular breeding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             paper_id        date  has_ai_policy  \\\n",
       "0    https://openalex.org/W4400975186  2024-07-25           True   \n",
       "1    https://openalex.org/W4404702677  2024-11-25          False   \n",
       "2    https://openalex.org/W4404494880  2024-11-18          False   \n",
       "3    https://openalex.org/W4376866715  2023-05-17          False   \n",
       "4    https://openalex.org/W4408155423  2025-03-04           True   \n",
       "..                                ...         ...            ...   \n",
       "112  https://openalex.org/W4323050332  2023-03-04           True   \n",
       "113  https://openalex.org/W4398223368  2024-05-22           True   \n",
       "114  https://openalex.org/W4404968551  2024-12-03           True   \n",
       "115  https://openalex.org/W4376611400  2023-05-15           True   \n",
       "116  https://openalex.org/W4396873924  2024-05-01           True   \n",
       "\n",
       "                     location             ai_tool  \\\n",
       "0    Declaration of Interests             ChatGPT   \n",
       "1            Acknowledgements              Claude   \n",
       "2            Acknowledgements              Claude   \n",
       "3            Acknowledgements               GPT-4   \n",
       "4         Contributor Section               GPT-4   \n",
       "..                        ...                 ...   \n",
       "112          Acknowledgements             ChatGPT   \n",
       "113          Acknowledgements  Grammarly, ChatGPT   \n",
       "114          Acknowledgements         ChatGPT-3.5   \n",
       "115          Acknowledgements             ChatGPT   \n",
       "116          Acknowledgements             ChatGPT   \n",
       "\n",
       "                                        journal_name  \\\n",
       "0                                           PLoS ONE   \n",
       "1                             Psychological Medicine   \n",
       "2                             The Historical Journal   \n",
       "3                             JMIR Medical Education   \n",
       "4                              BMC Medical Education   \n",
       "..                                               ...   \n",
       "112                       Journal of Medical Systems   \n",
       "113                                         PLoS ONE   \n",
       "114                  International Journal of Cancer   \n",
       "115  International Journal for Educational Integrity   \n",
       "116                               Molecular Breeding   \n",
       "\n",
       "                                         usage_purpose journal_location  \\\n",
       "0              evaluate the capabilities of AI systems          Methods   \n",
       "1                  English language editing assistance    Not Specified   \n",
       "2                                     revision process              NaN   \n",
       "3    case study to discuss opportunities and challe...    Not Specified   \n",
       "4                                           assistance          Methods   \n",
       "..                                                 ...              ...   \n",
       "112                        answering medical questions          Methods   \n",
       "113  language improvement, advanced grammar and sty...          Methods   \n",
       "114               language and readability improvement          Methods   \n",
       "115  assessing quality by comparing seminal ideas o...          Methods   \n",
       "116     refine the written content of this publication          Methods   \n",
       "\n",
       "        journal_category                                      journal_usage  \\\n",
       "0    Disclosure Required                            Text content generation   \n",
       "1          Not Mentioned                                                NaN   \n",
       "2                    NaN                                                NaN   \n",
       "3          Not Mentioned                                                NaN   \n",
       "4    Disclosure Required  AI assisted copy editing; Formatting changes; ...   \n",
       "..                   ...                                                ...   \n",
       "112  Disclosure Required                         General writing assistance   \n",
       "113  Disclosure Required                            Text content generation   \n",
       "114  Disclosure Required  General editing; Grammar improvement; Spelling...   \n",
       "115  Disclosure Required  AI assisted copy editing; Formatting changes; ...   \n",
       "116  Disclosure Required                         General writing assistance   \n",
       "\n",
       "                    journal_name_std  \n",
       "0                           plos one  \n",
       "1                  psychological med  \n",
       "2             the historical journal  \n",
       "3                 jmir med education  \n",
       "4                  bmc med education  \n",
       "..                               ...  \n",
       "112                    j med systems  \n",
       "113                         plos one  \n",
       "114                     int j cancer  \n",
       "115  int j for educational integrity  \n",
       "116               molecular breeding  \n",
       "\n",
       "[117 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始过滤和标准化usage_purpose字段...\n",
      "\n",
      "过滤前总行数: 117\n",
      "过滤后总行数: 77\n",
      "被过滤掉的行数: 40\n",
      "\n",
      "=== 过滤后usage_purpose分类统计 ===\n",
      "usage_purpose_categories_filtered\n",
      "Writing & Editing Support        28\n",
      "Language & Grammar Support       15\n",
      "Error Analysis & Others          14\n",
      "Content Creation & Generation    13\n",
      "Style & Formatting                4\n",
      "Reference & Citation Support      2\n",
      "Translation Services              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 被过滤掉的usage_purpose项目 ===\n",
      "usage_purpose\n",
      "Unspecified                                                                                  17\n",
      "evaluate the capabilities of AI systems                                                       1\n",
      "case study to discuss opportunities and challenges in medical education                       1\n",
      "processing and integrating complex data                                                       1\n",
      "Demonstration video                                                                           1\n",
      "capturing nuanced biases in peer review                                                       1\n",
      "Providing a quote                                                                             1\n",
      "data analysis                                                                                 1\n",
      "tagging and retrieval capabilities for efficient and thorough analysis                        1\n",
      "Enhancing social role and humaneness in interactions                                          1\n",
      "creating more natural conversations by logging input-response history                         1\n",
      "revising the authors’ written texts                                                           1\n",
      "querying about amblyopia and comparing answers                                                1\n",
      "analyzing political statements                                                                1\n",
      "Identifying healthcare needs                                                                  1\n",
      "response to the Home Blood Pressure Monitoring (HBPM) knowledge checklist                     1\n",
      "supporting the oncological workflow by helping confirm the absence or presence of lesions     1\n",
      "Quantifying confidence shifts                                                                 1\n",
      "tackle the side effects of PLEs from the learner and learning perspective                     1\n",
      "personalised learning                                                                         1\n",
      "integrating AI-driven components into curricula                                               1\n",
      "development of conversational technologies                                                    1\n",
      "answering medical questions                                                                   1\n",
      "assessing quality by comparing seminal ideas of assignments                                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "过滤后的结果已保存到 filtered_ai_usage_disclosure.csv\n",
      "开始标准化usage_purpose和journal_usage字段...\n",
      "\n",
      "=== usage_purpose标准化结果 ===\n",
      "usage_purpose_categories\n",
      "Error Analysis & Others          53\n",
      "Writing & Editing Support        29\n",
      "Language & Grammar Support       15\n",
      "Content Creation & Generation    13\n",
      "Style & Formatting                4\n",
      "Reference & Citation Support      2\n",
      "Translation Services              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== journal_usage标准化结果 ===\n",
      "Writing & Editing Support        65\n",
      "Language & Grammar Support       18\n",
      "Content Creation & Generation    15\n",
      "Style & Formatting               13\n",
      "Error Analysis & Others           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 前5行标准化示例 ===\n",
      "                                       usage_purpose  \\\n",
      "0            evaluate the capabilities of AI systems   \n",
      "1                English language editing assistance   \n",
      "2                                   revision process   \n",
      "3  case study to discuss opportunities and challe...   \n",
      "4                                         assistance   \n",
      "\n",
      "     usage_purpose_categories  \\\n",
      "0     Error Analysis & Others   \n",
      "1  Language & Grammar Support   \n",
      "2   Writing & Editing Support   \n",
      "3     Error Analysis & Others   \n",
      "4     Error Analysis & Others   \n",
      "\n",
      "                                       journal_usage  \\\n",
      "0                            Text content generation   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  AI assisted copy editing; Formatting changes; ...   \n",
      "\n",
      "                            journal_usage_categories  \n",
      "0                    [Content Creation & Generation]  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3                                                 []  \n",
      "4  [Style & Formatting, Language & Grammar Suppor...  \n",
      "\n",
      "结果已保存到 merged_df_with_standardized_usage.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Design comprehensive merge mapping - 8 major categories (for journal_usage field)\n",
    "comprehensive_merge_mapping = {\n",
    "    # 1. Language & Grammar Support\n",
    "    \"Grammar checking\": \"Language & Grammar Support\",\n",
    "    \"Grammar correction\": \"Language & Grammar Support\", \n",
    "    \"Grammar improvement\": \"Language & Grammar Support\",\n",
    "    \"Grammar enhancement\": \"Language & Grammar Support\",\n",
    "    \"Grammar suggestions\": \"Language & Grammar Support\",\n",
    "    \"Grammar\": \"Language & Grammar Support\",\n",
    "    \"Grammar and spelling check\": \"Language & Grammar Support\",\n",
    "    \"Grammar and style improvement\": \"Language & Grammar Support\",\n",
    "    \"Spelling and grammar improvement\": \"Language & Grammar Support\",\n",
    "    \"Spelling checking\": \"Language & Grammar Support\",\n",
    "    \"Spelling correction\": \"Language & Grammar Support\",\n",
    "    \"Spelling improvement\": \"Language & Grammar Support\",\n",
    "    \"Spelling assistance\": \"Language & Grammar Support\",\n",
    "    \"Spelling\": \"Language & Grammar Support\",\n",
    "    \"Spell checking\": \"Language & Grammar Support\",\n",
    "    \"Spelling check\": \"Language & Grammar Support\",\n",
    "    \"Spelling checks\": \"Language & Grammar Support\",\n",
    "    \"Language enhancement\": \"Language & Grammar Support\",\n",
    "    \"Language improvement\": \"Language & Grammar Support\",\n",
    "    \"Language polishing\": \"Language & Grammar Support\",\n",
    "    \"Language editing\": \"Language & Grammar Support\",\n",
    "    \"Language accuracy enhancement\": \"Language & Grammar Support\",\n",
    "    \"Improving language\": \"Language & Grammar Support\",\n",
    "    \"English improvement\": \"Language & Grammar Support\",\n",
    "    \"Linguistic quality enhancement\": \"Language & Grammar Support\",\n",
    "    \"Language barrier overcoming\": \"Language & Grammar Support\",\n",
    "    \"Language proofreading\": \"Language & Grammar Support\",\n",
    "    \n",
    "    # 2. Writing & Editing Support\n",
    "    \"General writing assistance\": \"Writing & Editing Support\",\n",
    "    \"Writing assistance\": \"Writing & Editing Support\",\n",
    "    \"Medical writing assistance\": \"Writing & Editing Support\",\n",
    "    \"General editing\": \"Writing & Editing Support\",\n",
    "    \"Editing\": \"Writing & Editing Support\",\n",
    "    \"AI assisted copy editing\": \"Writing & Editing Support\",\n",
    "    \"Text editing\": \"Writing & Editing Support\",\n",
    "    \"Content editing\": \"Writing & Editing Support\",\n",
    "    \"Editing optimization\": \"Writing & Editing Support\",\n",
    "    \"Editing of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"Editing of the text\": \"Writing & Editing Support\",\n",
    "    \"Editing text\": \"Writing & Editing Support\",\n",
    "    \"General text editing\": \"Writing & Editing Support\",\n",
    "    \"Light editing\": \"Writing & Editing Support\",\n",
    "    \"Writing editing\": \"Writing & Editing Support\",\n",
    "    \"Copyediting\": \"Writing & Editing Support\",\n",
    "    \"Editing tool usage\": \"Writing & Editing Support\",\n",
    "    \"Readability improvement\": \"Writing & Editing Support\",\n",
    "    \"Readability enhancement\": \"Writing & Editing Support\",\n",
    "    \"Clarity improvement\": \"Writing & Editing Support\",\n",
    "    \"Quality improvement\": \"Writing & Editing Support\",\n",
    "    \"Structure improvement\": \"Writing & Editing Support\",\n",
    "    \"Structure enhancement\": \"Writing & Editing Support\",\n",
    "    \"Sentence structure improvement\": \"Writing & Editing Support\",\n",
    "    \"Phrase suggestion\": \"Writing & Editing Support\",\n",
    "    \"Synonym suggestions\": \"Writing & Editing Support\",\n",
    "    \"Personalized feedback\": \"Writing & Editing Support\",\n",
    "    \"Accessibility enhancement\": \"Writing & Editing Support\",\n",
    "    \n",
    "    # 3. Style & Formatting\n",
    "    \"Style enhancement\": \"Style & Formatting\",\n",
    "    \"Style improvement\": \"Style & Formatting\",\n",
    "    \"Style and tone suggestions\": \"Style & Formatting\",\n",
    "    \"Tone adjustment\": \"Style & Formatting\",\n",
    "    \"Wording changes\": \"Style & Formatting\",\n",
    "    \"Formatting changes\": \"Style & Formatting\",\n",
    "    \"Formatting\": \"Style & Formatting\",\n",
    "    \"Punctuation correction\": \"Style & Formatting\",\n",
    "    \"Text formatting\": \"Style & Formatting\",\n",
    "    \n",
    "    # 4. Content Creation & Generation\n",
    "    \"Content generation\": \"Content Creation & Generation\",\n",
    "    \"Content creation\": \"Content Creation & Generation\",\n",
    "    \"Text generation\": \"Content Creation & Generation\",\n",
    "    \"Text content generation\": \"Content Creation & Generation\",\n",
    "    \"Text creation\": \"Content Creation & Generation\",\n",
    "    \"Abstract writing\": \"Content Creation & Generation\",\n",
    "    \"Drafting text\": \"Content Creation & Generation\",\n",
    "    \"Drafting of the manuscript\": \"Content Creation & Generation\",\n",
    "    \"Drafting of scientific manuscripts\": \"Content Creation & Generation\",\n",
    "    \"Writing of the manuscript\": \"Content Creation & Generation\",\n",
    "    \"Writing of a manuscript\": \"Content Creation & Generation\",\n",
    "    \"Manuscript preparation\": \"Content Creation & Generation\",\n",
    "    \"Plain-language summaries\": \"Content Creation & Generation\",\n",
    "    \"Literature review assistance\": \"Content Creation & Generation\",\n",
    "    \"Reporting standards\": \"Content Creation & Generation\",\n",
    "    \n",
    "    # 5. Reference & Citation Support\n",
    "    \"Reference checking\": \"Reference & Citation Support\",\n",
    "    \"Reference generation\": \"Reference & Citation Support\",\n",
    "    \"Reference management\": \"Reference & Citation Support\",\n",
    "    \"Reference organizing\": \"Reference & Citation Support\",\n",
    "    \"Reference clean up\": \"Reference & Citation Support\",\n",
    "    \"Reference list preparation\": \"Reference & Citation Support\",\n",
    "    \"Compilation of references\": \"Reference & Citation Support\",\n",
    "    \n",
    "    # 6. Translation Services\n",
    "    \"Translation\": \"Translation Services\",\n",
    "    \"Language translation\": \"Translation Services\",\n",
    "    \"Translation of own words\": \"Translation Services\",\n",
    "    \n",
    "    # 7. Text Processing & Refinement\n",
    "    \"Text correcting\": \"Text Processing & Refinement\",\n",
    "    \"Text refining\": \"Text Processing & Refinement\",\n",
    "    \"Text revision\": \"Text Processing & Refinement\",\n",
    "    \"Text incorporation\": \"Text Processing & Refinement\",\n",
    "    \"Text snippet inclusion\": \"Text Processing & Refinement\",\n",
    "    \"Paraphrasing\": \"Text Processing & Refinement\",\n",
    "    \"Summarizing\": \"Text Processing & Refinement\",\n",
    "    \"Condensing writing\": \"Text Processing & Refinement\",\n",
    "    \"Polishing writing\": \"Text Processing & Refinement\",\n",
    "    \"Revising\": \"Text Processing & Refinement\",\n",
    "    \"Reviewing\": \"Text Processing & Refinement\",\n",
    "    \n",
    "    # 8. Error Analysis & Others\n",
    "    \"Error in analysis\": \"Error Analysis & Others\",\n",
    "}\n",
    "\n",
    "\n",
    "# Redefine usage_purpose_mapping, only keeping real \"author using AI\" scenarios\n",
    "usage_purpose_mapping_filtered = {\n",
    "    # 1. Language & Grammar Support\n",
    "    \"English language editing assistance\": \"Language & Grammar Support\",\n",
    "    \"grammatical editing of the manuscript\": \"Language & Grammar Support\",\n",
    "    \"correcting grammatical errors\": \"Language & Grammar Support\",\n",
    "    \"improve the grammar and style of this paper\": \"Language & Grammar Support\",\n",
    "    \"polishing English language and grammatical check\": \"Language & Grammar Support\",\n",
    "    \"improve readability and grammar\": \"Language & Grammar Support\",\n",
    "    \"text editing (English language fluency, grammar, word choice, sentence structure)\": \"Language & Grammar Support\",\n",
    "    \"language checking in selected sentences\": \"Language & Grammar Support\",\n",
    "    \"proofreading to improve readability and language\": \"Language & Grammar Support\",\n",
    "    \"language and readability improvement\": \"Language & Grammar Support\",\n",
    "    \"language improvement\": \"Language & Grammar Support\",\n",
    "    \"checking and correcting English expressions\": \"Language & Grammar Support\",\n",
    "    \"improve writing\": \"Language & Grammar Support\",\n",
    "    \"English editing and checking the grammar\": \"Language & Grammar Support\",\n",
    "    \n",
    "    # 2. Writing & Editing Support\n",
    "    \"proofreading and editing the manuscript\": \"Writing & Editing Support\",\n",
    "    \"proofreading\": \"Writing & Editing Support\",\n",
    "    \"text editing\": \"Writing & Editing Support\",\n",
    "    \"revising the authors' written texts\": \"Writing & Editing Support\",\n",
    "    \"revision process\": \"Writing & Editing Support\",\n",
    "    \"improve the readability and conciseness of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"correction and improvement of scientific medical writing\": \"Writing & Editing Support\",\n",
    "    \"language editing and rephrasing\": \"Writing & Editing Support\",\n",
    "    \"language editing\": \"Writing & Editing Support\",\n",
    "    \"language revisions of intellectual content\": \"Writing & Editing Support\",\n",
    "    \"refine the written content of this publication\": \"Writing & Editing Support\",\n",
    "    \"enhance the language and readability of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"improve the clarity and language of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"improve the readability and language of some paragraphs in the text\": \"Writing & Editing Support\",\n",
    "    \"language editing and improving the clarity of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"improve language\": \"Writing & Editing Support\",\n",
    "    \"identify improvements in the writing style\": \"Writing & Editing Support\",\n",
    "    \"editorial purposes in improving the clarity and language of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"enhance the clarity of select portions of the text\": \"Writing & Editing Support\",\n",
    "    \"proofreading and copyediting\": \"Writing & Editing Support\",\n",
    "    \"revising sentences, improving grammar and enriching the vocabulary\": \"Writing & Editing Support\",\n",
    "    \"improve the clarity, coherence, and overall presentation of the manuscript\": \"Writing & Editing Support\",\n",
    "    \"improving readability and fitting the length of the article\": \"Writing & Editing Support\",\n",
    "    \"language improvement, advanced grammar and style checks, refining manuscript's language and ensuring readability\": \"Writing & Editing Support\",\n",
    "    \n",
    "    # 3. Style & Formatting\n",
    "    \"formatting\": \"Style & Formatting\",\n",
    "    \"creating the graphical abstract\": \"Style & Formatting\",\n",
    "    \"refining phrasing and brainstorming alternative title suggestions\": \"Style & Formatting\",\n",
    "    \"phrasing of the manuscript\": \"Style & Formatting\",\n",
    "    \n",
    "    # 4. Content Creation & Generation\n",
    "    \"text generation\": \"Content Creation & Generation\",\n",
    "    \"content generation\": \"Content Creation & Generation\",\n",
    "    \"poem generation\": \"Content Creation & Generation\",\n",
    "    \"TOC generation\": \"Content Creation & Generation\",\n",
    "    \"creating a convincing looking scientific abstract or article\": \"Content Creation & Generation\",\n",
    "    \"co-authorship\": \"Content Creation & Generation\",\n",
    "    \"summarized content\": \"Content Creation & Generation\",\n",
    "    \"proposing titles, structuring papers, crafting abstracts, and summarizing research\": \"Content Creation & Generation\",\n",
    "    \"creation of this manuscript\": \"Content Creation & Generation\",\n",
    "    \"writing certain parts of the article\": \"Content Creation & Generation\",\n",
    "    \n",
    "    # 5. Reference & Citation Support\n",
    "    \"classifying citation statements\": \"Reference & Citation Support\",\n",
    "    \"grammar, structure, citations, and adherence to disciplinary standards\": \"Reference & Citation Support\",\n",
    "    \n",
    "    # 6. Translation Services\n",
    "    \"English text translation and extensive manuscript proofreading and revision\": \"Translation Services\",\n",
    "    \n",
    "    # 7. Text Processing & Refinement\n",
    "    \"paraphrasing\": \"Text Processing & Refinement\",\n",
    "    \"summarizing\": \"Text Processing & Refinement\", \n",
    "    \"condensing writing\": \"Text Processing & Refinement\",\n",
    "    \"text refining\": \"Text Processing & Refinement\",\n",
    "    \"text revision\": \"Text Processing & Refinement\",\n",
    "    \n",
    "    # 8. Error Analysis & Others - only keep real \"author using AI\" scenarios\n",
    "    \"assistance\": \"Error Analysis & Others\",  # ambiguous but possibly usage\n",
    "    \"assisting in the preparation of Python codes\": \"Error Analysis & Others\",  # author using AI to write code\n",
    "    \"assist CAD design for microfluidic devices\": \"Error Analysis & Others\",  # author using AI for design\n",
    "    \"designing true–false questions\": \"Error Analysis & Others\",  # author using AI to generate questions\n",
    "    \"generating SCTs for comparison with clinical experts\": \"Error Analysis & Others\",  # author using AI to generate content\n",
    "    \"debugging certain codes\": \"Error Analysis & Others\",  # author using AI to debug code\n",
    "    \"custom fine-tuning\": \"Error Analysis & Others\",  # author using AI\n",
    "    \"writing a 210 multi choice questions-MCQs examination\": \"Error Analysis & Others\",  # author using AI to write questions\n",
    "    \"assigning categories to student responses\": \"Error Analysis & Others\",  # author using AI to classify\n",
    "    \"produce answers for each type of exam\": \"Error Analysis & Others\",  # author using AI to generate answers\n",
    "    \"support the writing of R and Python3 codes and edits throughout the manuscript\": \"Error Analysis & Others\",  # author using AI to write code\n",
    "    \"Generating artificial persona\": \"Error Analysis & Others\",  # author using AI to generate personas\n",
    "    \"Experiments\": \"Error Analysis & Others\",  # author using AI in experiments\n",
    "}\n",
    "\n",
    "# Items that need to be filtered out (these are not authors using AI, but research about AI or AI as research subject)\n",
    "excluded_usage_purposes = [\n",
    "    \"evaluate the capabilities of AI systems\",  # evaluating AI system capabilities\n",
    "    \"case study to discuss opportunities and challenges in medical education\",  # using AI as case study\n",
    "    \"processing and integrating complex data\",  # AI functionality description\n",
    "    \"Demonstration video\",  # demonstration video\n",
    "    \"capturing nuanced biases in peer review\",  # researching AI capabilities\n",
    "    \"Providing a quote\",  # AI providing quotes, not author usage\n",
    "    \"data analysis\",  # possibly AI in research\n",
    "    \"tagging and retrieval capabilities for efficient and thorough analysis\",  # researching AI capabilities\n",
    "    \"Enhancing social role and humaneness in interactions\",  # research content\n",
    "    \"creating more natural conversations by logging input-response history\",  # research content\n",
    "    \"querying about amblyopia and comparing answers\",  # research content\n",
    "    \"analyzing political statements\",  # research content\n",
    "    \"Identifying healthcare needs\",  # research content\n",
    "    \"response to the Home Blood Pressure Monitoring (HBPM) knowledge checklist\",  # research content\n",
    "    \"supporting the oncological workflow by helping confirm the absence or presence of lesions\",  # AI tool functionality description\n",
    "    \"Quantifying confidence shifts\",  # research content\n",
    "    \"tackle the side effects of PLEs from the learner and learning perspective\",  # research content\n",
    "    \"personalised learning\",  # research content\n",
    "    \"integrating AI-driven components into curricula\",  # research content\n",
    "    \"development of conversational technologies\",  # research content\n",
    "    \"answering medical questions\",  # research content\n",
    "    \"assessing quality by comparing seminal ideas of assignments\",  # research content\n",
    "    \"Unspecified\",  # unspecified\n",
    "]\n",
    "\n",
    "def categorize_usage_purpose_filtered(text):\n",
    "    \"\"\"Categorize usage_purpose, filter out non-AI usage scenarios\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None  # return None indicates need to be filtered\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    if not text or text.lower() == 'unspecified':\n",
    "        return None  # filter out\n",
    "    \n",
    "    # Check if in exclusion list\n",
    "    if text in excluded_usage_purposes:\n",
    "        return None  # filter out\n",
    "    \n",
    "    # Direct mapping\n",
    "    if text in usage_purpose_mapping_filtered:\n",
    "        return usage_purpose_mapping_filtered[text]\n",
    "    \n",
    "    # Fuzzy matching (only for clear AI usage scenarios)\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Language & Grammar Support\n",
    "    if any(word in text_lower for word in [\n",
    "        'grammar', 'spelling', 'language editing', 'english', 'linguistic', \n",
    "        'grammatical', 'language improvement', 'language check', 'proofreading'\n",
    "    ]):\n",
    "        return \"Language & Grammar Support\"\n",
    "    \n",
    "    # Writing & Editing Support  \n",
    "    elif any(word in text_lower for word in [\n",
    "        'editing', 'readability', 'clarity', 'writing assistance', \n",
    "        'revision', 'manuscript editing', 'text editing', 'language revision'\n",
    "    ]):\n",
    "        return \"Writing & Editing Support\"\n",
    "    \n",
    "    # Style & Formatting\n",
    "    elif any(word in text_lower for word in [\n",
    "        'formatting', 'style', 'phrasing', 'tone', 'graphical', 'title'\n",
    "    ]):\n",
    "        return \"Style & Formatting\"\n",
    "    \n",
    "    # Content Creation & Generation\n",
    "    elif any(word in text_lower for word in [\n",
    "        'generation', 'creation', 'writing content', 'content creation', 'abstract writing', \n",
    "        'manuscript writing', 'text generation', 'co-author'\n",
    "    ]):\n",
    "        return \"Content Creation & Generation\"\n",
    "    \n",
    "    # Reference & Citation Support\n",
    "    elif any(word in text_lower for word in [\n",
    "        'citation', 'reference', 'bibliography'\n",
    "    ]):\n",
    "        return \"Reference & Citation Support\"\n",
    "    \n",
    "    # Translation Services\n",
    "    elif any(word in text_lower for word in [\n",
    "        'translation', 'translate'\n",
    "    ]):\n",
    "        return \"Translation Services\"\n",
    "    \n",
    "    # Other clear AI usage scenarios\n",
    "    elif any(phrase in text_lower for phrase in [\n",
    "        'assist', 'help', 'support', 'aid', 'code', 'programming', 'debug'\n",
    "    ]):\n",
    "        return \"Error Analysis & Others\"\n",
    "    \n",
    "    else:\n",
    "        return None  # filter out uncertain items\n",
    "\n",
    "# Re-apply filtered classification\n",
    "print(\"Starting to filter and standardize usage_purpose field...\")\n",
    "\n",
    "# Apply filtered classification\n",
    "merged_df['usage_purpose_categories_filtered'] = merged_df['usage_purpose'].apply(categorize_usage_purpose_filtered)\n",
    "\n",
    "# Filter out rows classified as None (i.e., not real AI usage rows)\n",
    "filtered_df = merged_df[merged_df['usage_purpose_categories_filtered'].notna()].copy()\n",
    "\n",
    "print(f\"\\nTotal rows before filtering: {len(merged_df)}\")\n",
    "print(f\"Total rows after filtering: {len(filtered_df)}\")\n",
    "print(f\"Rows filtered out: {len(merged_df) - len(filtered_df)}\")\n",
    "\n",
    "# View filtered results\n",
    "print(\"\\n=== Filtered usage_purpose classification statistics ===\")\n",
    "purpose_counts_filtered = filtered_df['usage_purpose_categories_filtered'].value_counts()\n",
    "print(purpose_counts_filtered)\n",
    "\n",
    "# View filtered out items\n",
    "excluded_items = merged_df[merged_df['usage_purpose_categories_filtered'].isna()]['usage_purpose'].value_counts()\n",
    "if len(excluded_items) > 0:\n",
    "    print(f\"\\n=== Filtered out usage_purpose items ===\")\n",
    "    print(excluded_items)\n",
    "\n",
    "# Continue processing journal_usage field (keep original logic)\n",
    "# filtered_df['journal_usage_categories'] = filtered_df['journal_usage_items'].apply(categorize_journal_usage)\n",
    "\n",
    "# print(\"\\n=== First 5 rows after filtering ===\")\n",
    "# example_cols = ['usage_purpose', 'usage_purpose_categories_filtered', 'journal_usage', 'journal_usage_categories']\n",
    "# print(filtered_df[example_cols].head())\n",
    "\n",
    "# Save filtered results\n",
    "filtered_df.to_csv('filtered_ai_usage_disclosure.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nFiltered results saved to filtered_ai_usage_disclosure.csv\")\n",
    "\n",
    "def split_and_clean_text(text, separators=[';', ',', ' and ', '&']):\n",
    "    \"\"\"Split and clean text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Use multiple separators to split\n",
    "    items = [text]\n",
    "    for sep in separators:\n",
    "        new_items = []\n",
    "        for item in items:\n",
    "            new_items.extend([x.strip() for x in item.split(sep) if x.strip()])\n",
    "        items = new_items\n",
    "    \n",
    "    return [item for item in items if item and item.lower() != 'unspecified']\n",
    "\n",
    "def categorize_usage_purpose(text):\n",
    "    \"\"\"Categorize usage_purpose\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"Error Analysis & Others\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    if not text or text.lower() == 'unspecified':\n",
    "        return \"Error Analysis & Others\"\n",
    "    \n",
    "    # Direct mapping\n",
    "    if text in usage_purpose_mapping:\n",
    "        return usage_purpose_mapping[text]\n",
    "    \n",
    "    # Fuzzy matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Language & Grammar Support\n",
    "    if any(word in text_lower for word in [\n",
    "        'grammar', 'spelling', 'language editing', 'english', 'linguistic', \n",
    "        'grammatical', 'language improvement', 'language check'\n",
    "    ]):\n",
    "        return \"Language & Grammar Support\"\n",
    "    \n",
    "    # Writing & Editing Support  \n",
    "    elif any(word in text_lower for word in [\n",
    "        'proofreading', 'editing', 'readability', 'clarity', 'writing', \n",
    "        'revision', 'manuscript', 'text editing', 'language revision'\n",
    "    ]):\n",
    "        return \"Writing & Editing Support\"\n",
    "    \n",
    "    # Style & Formatting\n",
    "    elif any(word in text_lower for word in [\n",
    "        'formatting', 'style', 'phrasing', 'tone', 'graphical', 'title'\n",
    "    ]):\n",
    "        return \"Style & Formatting\"\n",
    "    \n",
    "    # Content Creation & Generation\n",
    "    elif any(word in text_lower for word in [\n",
    "        'generation', 'creation', 'writing', 'content', 'abstract', \n",
    "        'manuscript', 'text generation', 'co-author'\n",
    "    ]):\n",
    "        return \"Content Creation & Generation\"\n",
    "    \n",
    "    # Reference & Citation Support\n",
    "    elif any(word in text_lower for word in [\n",
    "        'citation', 'reference', 'bibliography'\n",
    "    ]):\n",
    "        return \"Reference & Citation Support\"\n",
    "    \n",
    "    # Translation Services\n",
    "    elif any(word in text_lower for word in [\n",
    "        'translation', 'translate'\n",
    "    ]):\n",
    "        return \"Translation Services\"\n",
    "    \n",
    "    else:\n",
    "        return \"Error Analysis & Others\"\n",
    "\n",
    "def categorize_journal_usage(usages):\n",
    "    \"\"\"Categorize journal_usage (based on existing mapping)\"\"\"\n",
    "    if not usages:\n",
    "        return []\n",
    "    \n",
    "    categories = set()\n",
    "    for usage in usages:\n",
    "        usage = usage.strip()\n",
    "        if usage in comprehensive_merge_mapping:\n",
    "            categories.add(comprehensive_merge_mapping[usage])\n",
    "        else:\n",
    "            # Automatic classification logic\n",
    "            usage_lower = usage.lower()\n",
    "            if any(word in usage_lower for word in ['grammar', 'spelling', 'language']):\n",
    "                categories.add(\"Language & Grammar Support\")\n",
    "            elif any(word in usage_lower for word in ['writing', 'editing', 'quality', 'clarity', 'readability']):\n",
    "                categories.add(\"Writing & Editing Support\")\n",
    "            elif any(word in usage_lower for word in ['style', 'tone', 'format', 'punctuation']):\n",
    "                categories.add(\"Style & Formatting\")\n",
    "            elif any(word in usage_lower for word in ['content', 'generation', 'creation', 'drafting', 'manuscript']):\n",
    "                categories.add(\"Content Creation & Generation\")\n",
    "            elif any(word in usage_lower for word in ['reference', 'citation']):\n",
    "                categories.add(\"Reference & Citation Support\")\n",
    "            elif any(word in usage_lower for word in ['translation', 'translate']):\n",
    "                categories.add(\"Translation Services\")\n",
    "            elif any(word in usage_lower for word in ['text', 'revision', 'refining', 'polishing', 'paraphras', 'summar']):\n",
    "                categories.add(\"Text Processing & Refinement\")\n",
    "            else:\n",
    "                categories.add(\"Error Analysis & Others\")\n",
    "    \n",
    "    return list(categories)\n",
    "\n",
    "# Apply standardization\n",
    "print(\"Starting to standardize usage_purpose and journal_usage fields...\")\n",
    "\n",
    "# 1. Standardize usage_purpose field\n",
    "merged_df['usage_purpose_items'] = merged_df['usage_purpose'].apply(\n",
    "    lambda x: split_and_clean_text(x, [',', ' and ', '&', ';'])\n",
    ")\n",
    "\n",
    "merged_df['usage_purpose_categories'] = merged_df['usage_purpose'].apply(categorize_usage_purpose)\n",
    "\n",
    "# 2. Standardize journal_usage field\n",
    "merged_df['journal_usage_items'] = merged_df['journal_usage'].apply(\n",
    "    lambda x: split_and_clean_text(x, [';']) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "merged_df['journal_usage_categories'] = merged_df['journal_usage_items'].apply(categorize_journal_usage)\n",
    "\n",
    "# View results\n",
    "print(\"\\n=== usage_purpose standardization results ===\")\n",
    "purpose_counts = merged_df['usage_purpose_categories'].value_counts()\n",
    "print(purpose_counts)\n",
    "\n",
    "print(\"\\n=== journal_usage standardization results ===\")\n",
    "# Expand journal_usage_categories for statistics\n",
    "all_journal_categories = []\n",
    "for categories in merged_df['journal_usage_categories']:\n",
    "    all_journal_categories.extend(categories)\n",
    "\n",
    "journal_usage_counts = pd.Series(all_journal_categories).value_counts()\n",
    "print(journal_usage_counts)\n",
    "\n",
    "# View first few rows example\n",
    "print(\"\\n=== First 5 rows standardization example ===\")\n",
    "example_cols = ['usage_purpose', 'usage_purpose_categories', 'journal_usage', 'journal_usage_categories']\n",
    "print(merged_df[example_cols].head())\n",
    "\n",
    "# Save results\n",
    "# merged_df.to_csv('merged_df_with_standardized_usage.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nResults saved to merged_df_with_standardized_usage.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
